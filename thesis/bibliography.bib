@misc{widsdatathon2024-challenge2,
	annote = {Kaggle},
	author = {{Women in Data Science}},
	title = {{WiDS Datathon 2024 Challenge #2}},
	url = {https://kaggle.com/competitions/widsdatathon2024-challenge2},
	year = {2024}
}

@article{10.1145/2500499,
	author = {Dhar, Vasant},
	title = {Data science and prediction},
	year = {2013},
	issue_date = {December 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {56},
	number = {12},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2500499},
	doi = {10.1145/2500499},
	abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
	journal = {Commun. ACM},
	month = dec,
	pages = {64–73},
	numpages = {10}
}

@article{Donoho02102017,
	author = {David Donoho and},
	title = {50 Years of Data Science},
	journal = {Journal of Computational and Graphical Statistics},
	volume = {26},
	number = {4},
	pages = {745--766},
	year = {2017},
	publisher = {ASA Website},
	doi = {10.1080/10618600.2017.1384734},
	URL = { 
	
	https://doi.org/10.1080/10618600.2017.1384734
	},
	eprint = { 
	
	https://doi.org/10.1080/10618600.2017.1384734
	}
	
}

@article{datasciencelifecycle,
	author = {Stodden, Victoria},
	title = {The data science life cycle: a disciplined approach to advancing data science as a science},
	year = {2020},
	issue_date = {July 2020},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {63},
	number = {7},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3360646},
	doi = {10.1145/3360646},
	abstract = {A cycle that traces ways to define the landscape of data science.},
	journal = {Commun. ACM},
	month = jun,
	pages = {58–66},
	numpages = {9}
}

@article{shearer2000crisp,
	added-at = {2018-05-15T21:48:43.000+0200},
	author = {Shearer, Colin},
	biburl = {https://www.bibsonomy.org/bibtex/24e676fa2d25f47a2c4937c781a1b0106/becker},
	description = {https://scholar.googleusercontent.com/scholar.bib?q=info:I9jFunQhVmEJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWvs71DlbwrFoWMlTh6YkK1XX1u-dGdqu&scisf=4&ct=citation&cd=0&hl=de},
	interhash = {5d61c7884c690a61453e519ef8f5de38},
	intrahash = {4e676fa2d25f47a2c4937c781a1b0106},
	journal = {Journal of data warehousing},
	keywords = {crisp data deepscan dm mining},
	number = 4,
	pages = {13--22},
	publisher = {THE DATA WAREHOUSE INSTITUTE},
	timestamp = {2018-05-15T21:48:43.000+0200},
	title = {The CRISP-DM model: the new blueprint for data mining},
	volume = 5,
	year = 2000
}

@article{Wing2019Data,
	author = {Wing, Jeannette M.},
	journal = {Harvard Data Science Review},
	number = {1},
	year = {2019},
	month = {jul 1},
	note = {https://hdsr.mitpress.mit.edu/pub/577rq08d},
	publisher = {The MIT Press},
	title = {The {Data} {Life} {Cycle}},
	volume = {1},
}

@article{potential,
	author = {Berman, Francine and Rutenbar, Rob and Hailpern, Brent and Christensen, Henrik and Davidson, Susan and Estrin, Deborah and Franklin, Michael and Martonosi, Margaret and Raghavan, Padma and Stodden, Victoria and Szalay, Alexander S.},
	title = {Realizing the potential of data science},
	year = {2018},
	issue_date = {April 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {61},
	number = {4},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3188721},
	doi = {10.1145/3188721},
	abstract = {Data science promises new insights, helping transform information into knowledge that can drive science and industry.},
	journal = {Commun. ACM},
	month = mar,
	pages = {67–72},
	numpages = {6}
}

@book{aima,
	author = {Russell, Stuart and Norvig, Peter},
	title = {Artificial Intelligence: A Modern Approach},
	year = {2009},
	isbn = {0136042597},
	publisher = {Prentice Hall Press},
	address = {USA},
	edition = {3rd},
	abstract = {The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.}
}

@inbook{eda,
	author = {Komorowski, Matthieu and Marshall, Dominic and Salciccioli, Justin and Crutain, Yves},
	year = {2016},
	month = {09},
	pages = {185-203},
	title = {Exploratory Data Analysis},
	isbn = {978-3-319-43740-8},
	journal = {Secondary Analysis of Electronic Health Records},
	doi = {10.1007/978-3-319-43742-2_15}
}

@article{Mariscal_Marbán_Fernández_2010, title={A survey of data mining and knowledge discovery process models and methodologies}, volume={25}, DOI={10.1017/S0269888910000032}, number={2}, journal={The Knowledge Engineering Review}, author={Mariscal, Gonzalo and Marbán, Óscar and Fernández, Covadonga}, year={2010}, pages={137–166}}

@misc{datasciencepmCRISPDMStill,
	author = {Jeff Saltz},
	title = {{C}{R}{I}{S}{P}-{D}{M} is {S}till the {M}ost {P}opular {F}ramework for {E}xecuting {D}ata {S}cience {P}rojects - {D}ata {S}cience {P}{M} --- datascience-pm.com},
	howpublished = {\url{https://www.datascience-pm.com/crisp-dm-still-most-popular/}},
	year = {2024},
	note = {[Accessed 28-05-2025]},
}

@book{mitchell1997machine,
	added-at = {2022-09-08T09:41:39.000+0200},
	author = {Mitchell, Tom M},
	biburl = {https://www.bibsonomy.org/bibtex/2ea9f893d9d19c182bcf2822eb590fe4f/msteininger},
	interhash = {479a66c32badb3a455fbdcf8e6633a5d},
	intrahash = {ea9f893d9d19c182bcf2822eb590fe4f},
	keywords = {book ml},
	number = 9,
	publisher = {McGraw-hill New York},
	timestamp = {2022-09-08T17:32:10.000+0200},
	title = {Machine learning},
	volume = 1,
	year = 1997
}

@book{mlprobabilistic,
	author = {Murphy, Kevin P.},
	title = {Machine Learning: A Probabilistic Perspective},
	year = {2012},
	isbn = {0262018020},
	publisher = {The MIT Press},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.}
}

@misc{tai2021surveyregressionalgorithmsconnections,
	title={A Survey Of Regression Algorithms And Connections With Deep Learning}, 
	author={Yunpeng Tai},
	year={2021},
	eprint={2104.12647},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2104.12647}, 
}

@book{Goodfellow-et-al-2016,
	added-at = {2017-03-13T20:27:27.000+0100},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	biburl = {https://www.bibsonomy.org/bibtex/2175f81afff897a68829e4d30c080a8fb/hotho},
	interhash = {62814dec510d5c55b0b38ad85a6c748d},
	intrahash = {175f81afff897a68829e4d30c080a8fb},
	keywords = {book deep learning toread},
	note = {Book in preparation for MIT Press},
	publisher = {MIT Press},
	timestamp = {2017-04-14T13:44:20.000+0200},
	title = {Deep Learning},
	url = {http://www.deeplearningbook.org},
	year = 2016
}

@book{patternrecognition,
	author = {Bishop, Christopher M.},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006},
	isbn = {0387310738},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg}
}

@book{Burkov2019TheHM,
	title={The Hundred-Page Machine Learning Book},
	author={Andriy Burkov},
	year={2019},
	url={https://api.semanticscholar.org/CorpusID:202780305}
}

@inproceedings{l1l2,
	author = {Ng, Andrew Y.},
	title = {Feature selection, L1 vs. L2 regularization, and rotational invariance},
	year = {2004},
	isbn = {1581138385},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1015330.1015435},
	doi = {10.1145/1015330.1015435},
	abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
	booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
	pages = {78},
	location = {Banff, Alberta, Canada},
	series = {ICML '04}
}

@article{lasso,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	author = {Robert Tibshirani},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {267--288},
	publisher = {[Royal Statistical Society, Oxford University Press]},
	title = {Regression Shrinkage and Selection via the Lasso},
	urldate = {2025-05-30},
	volume = {58},
	year = {1996}
}

@article{elasticnet,
	author = {Zou, Hui and Hastie, Trevor},
	title = {Regularization and Variable Selection Via the Elastic Net},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	volume = {67},
	number = {2},
	pages = {301-320},
	year = {2005},
	month = {03},
	abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	issn = {1369-7412},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	url = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
}

@article{ridge,
	ISSN = {00401706},
	URL = {http://www.jstor.org/stable/1271436},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	author = {Arthur E. Hoerl and Robert W. Kennard},
	journal = {Technometrics},
	number = {1},
	pages = {80--86},
	publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
	title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
	urldate = {2025-05-30},
	volume = {42},
	year = {2000}
}

@article{svr,
	author = {Smola, Alex and Schölkopf, Bernhard},
	year = {2004},
	month = {08},
	pages = {199-222},
	title = {A tutorial on support vector regression},
	volume = {14},
	journal = {Statistics and Computing},
	doi = {10.1023/B%3ASTCO.0000035301.49549.88}
}

@inproceedings{Chen_2016, series={KDD ’16},
	title={XGBoost: A Scalable Tree Boosting System},
	url={http://dx.doi.org/10.1145/2939672.2939785},
	DOI={10.1145/2939672.2939785},
	booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	publisher={ACM},
	author={Chen, Tianqi and Guestrin, Carlos},
	year={2016},
	month=aug, pages={785–794},
	collection={KDD ’16} }

@misc{dorogush2018catboostgradientboostingcategorical,
	title={CatBoost: gradient boosting with categorical features support}, 
	author={Anna Veronika Dorogush and Vasily Ershov and Andrey Gulin},
	year={2018},
	eprint={1810.11363},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1810.11363}, 
}

@inproceedings{NIPS2017_6449f44a,
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
	volume = {30},
	year = {2017}
}

@inbook{histboost,
	author = {Guryanov, Aleksei},
	year = {2019},
	month = {12},
	pages = {39-50},
	title = {Histogram-Based Algorithm for Building Gradient Boosting Ensembles of Piecewise Linear Decision Trees},
	isbn = {9783030373337},
	doi = {10.1007/978-3-030-37334-4_4}
}

@inproceedings{adaboost,
	author = {Freund, Yoav and Schapire, Robert E.},
	title = {A decision-theoretic generalization of on-line learning and an application to boosting},
	year = {1995},
	isbn = {3540591192},
	booktitle = {Proceedings of the Second European Conference on Computational Learning Theory},
	pages = {23–37},
	numpages = {15},
	series = {EuroCOLT '95}
}


@article{baggingart,
	author = {Breiman, Leo},
	title = {Bagging predictors},
	year = {1996},
	issue_date = {Aug. 1996},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {24},
	number = {2},
	issn = {0885-6125},
	url = {https://doi.org/10.1023/A:1018054314350},
	doi = {10.1023/A:1018054314350},
	journal = {Mach. Learn.},
	month = aug,
	pages = {123–140},
	numpages = {18},
	keywords = {aggregation, averaging, bootstrap, combining}
}

@article{randomforests,
	author = {Breiman, Leo},
	title = {Random Forests},
	year = {2001},
	issue_date = {October 1 2001},
	publisher = {Kluwer Academic Publishers},
	address = {USA},
	volume = {45},
	number = {1},
	issn = {0885-6125},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	journal = {Mach. Learn.},
	month = oct,
	pages = {5–32},
	numpages = {28},
	keywords = {regression, ensemble, classification}
}

@Article{Geurts2006,
	author={Geurts, Pierre
	and Ernst, Damien
	and Wehenkel, Louis},
	title={Extremely randomized trees},
	journal={Machine Learning},
	year={2006},
	month={Apr},
	day={01},
	volume={63},
	number={1},
	pages={3-42},
	abstract={This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
	issn={1573-0565},
	doi={10.1007/s10994-006-6226-1},
	url={https://doi.org/10.1007/s10994-006-6226-1}
}

@article{gradientboosting,
	author = {Jerome H. Friedman},
	title = {{Greedy function approximation: A gradient boosting machine.}},
	volume = {29},
	journal = {The Annals of Statistics},
	number = {5},
	publisher = {Institute of Mathematical Statistics},
	pages = {1189 -- 1232},
	keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
	year = {2001},
	doi = {10.1214/aos/1013203451},
	URL = {https://doi.org/10.1214/aos/1013203451}
}

@misc{catboost2,
	title={CatBoost: unbiased boosting with categorical features}, 
	author={Liudmila Prokhorenkova and Gleb Gusev and Aleksandr Vorobev and Anna Veronika Dorogush and Andrey Gulin},
	year={2019},
	eprint={1706.09516},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1706.09516}, 
}

@article{scikit-learn,
	title={Scikit-learn: Machine Learning in {P}ython},
	author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
	and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
	and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
	Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	journal={Journal of Machine Learning Research},
	volume={12},
	pages={2825--2830},
	year={2011}
}

@article{featureselection,
	author = {Guyon, Isabelle and Elisseeff, André},
	year = {2003},
	month = {01},
	pages = {1157 - 1182},
	title = {An Introduction of Variable and Feature Selection},
	volume = {3},
	journal = {J. Machine Learning Research Special Issue on Variable and Feature Selection},
	doi = {10.1162/153244303322753616}
}

@book{GVK022791892,
	added-at = {2009-08-21T12:31:04.000+0200},
	address = {New York [u.a.]},
	author = {Draper, {Norman Richard} and Smith, {Harry}},
	biburl = {https://www.bibsonomy.org/bibtex/2866e071d36308fbf2229cfa0e2f21c3a/fbw_hannover},
	interhash = {4df04bbecfa219dfa49c6fe28b78f462},
	intrahash = {866e071d36308fbf2229cfa0e2f21c3a},
	isbn = {0471221708},
	keywords = {Mathematische_Statistik Methode Statistik},
	pagetotal = {IX, 407},
	ppn_gvk = {022791892},
	publisher = {Wiley},
	series = {Wiley series in probability and mathematical statistics},
	timestamp = {2009-08-21T12:31:07.000+0200},
	title = {Applied regression analysis},
	url = {http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&IKT=1016&TRM=ppn+022791892&sourceid=fbw_bibsonomy},
	year = 1966
}

