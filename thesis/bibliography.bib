@misc{widsdatathon2024-challenge2,
	annote = {Kaggle},
	author = {{Women in Data Science}},
	title = {{WiDS Datathon 2024 Challenge #2}},
	url = {https://kaggle.com/competitions/widsdatathon2024-challenge2},
	year = {2024}
}

@article{10.1145/2500499,
	author = {Dhar, Vasant},
	title = {Data science and prediction},
	year = {2013},
	issue_date = {December 2013},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {56},
	number = {12},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/2500499},
	doi = {10.1145/2500499},
	abstract = {Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.},
	journal = {Commun. ACM},
	month = dec,
	pages = {64–73},
	numpages = {10}
}

@article{Donoho02102017,
	author = {David Donoho and},
	title = {50 Years of Data Science},
	journal = {Journal of Computational and Graphical Statistics},
	volume = {26},
	number = {4},
	pages = {745--766},
	year = {2017},
	publisher = {ASA Website},
	doi = {10.1080/10618600.2017.1384734},
	URL = { 
	
	https://doi.org/10.1080/10618600.2017.1384734
	},
	eprint = { 
	
	https://doi.org/10.1080/10618600.2017.1384734
	}
	
}

@article{datasciencelifecycle,
	author = {Stodden, Victoria},
	title = {The data science life cycle: a disciplined approach to advancing data science as a science},
	year = {2020},
	issue_date = {July 2020},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {63},
	number = {7},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3360646},
	doi = {10.1145/3360646},
	abstract = {A cycle that traces ways to define the landscape of data science.},
	journal = {Commun. ACM},
	month = jun,
	pages = {58–66},
	numpages = {9}
}

@article{shearer2000crisp,
	added-at = {2018-05-15T21:48:43.000+0200},
	author = {Shearer, Colin},
	biburl = {https://www.bibsonomy.org/bibtex/24e676fa2d25f47a2c4937c781a1b0106/becker},
	description = {https://scholar.googleusercontent.com/scholar.bib?q=info:I9jFunQhVmEJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWvs71DlbwrFoWMlTh6YkK1XX1u-dGdqu&scisf=4&ct=citation&cd=0&hl=de},
	interhash = {5d61c7884c690a61453e519ef8f5de38},
	intrahash = {4e676fa2d25f47a2c4937c781a1b0106},
	journal = {Journal of data warehousing},
	keywords = {crisp data deepscan dm mining},
	number = 4,
	pages = {13--22},
	publisher = {THE DATA WAREHOUSE INSTITUTE},
	timestamp = {2018-05-15T21:48:43.000+0200},
	title = {The CRISP-DM model: the new blueprint for data mining},
	volume = 5,
	year = 2000
}

@article{Wing2019Data,
	author = {Wing, Jeannette M.},
	journal = {Harvard Data Science Review},
	number = {1},
	year = {2019},
	month = {jul 1},
	note = {https://hdsr.mitpress.mit.edu/pub/577rq08d},
	publisher = {The MIT Press},
	title = {The {Data} {Life} {Cycle}},
	volume = {1},
}

@article{potential,
	author = {Berman, Francine and Rutenbar, Rob and Hailpern, Brent and Christensen, Henrik and Davidson, Susan and Estrin, Deborah and Franklin, Michael and Martonosi, Margaret and Raghavan, Padma and Stodden, Victoria and Szalay, Alexander S.},
	title = {Realizing the potential of data science},
	year = {2018},
	issue_date = {April 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {61},
	number = {4},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/3188721},
	doi = {10.1145/3188721},
	abstract = {Data science promises new insights, helping transform information into knowledge that can drive science and industry.},
	journal = {Commun. ACM},
	month = mar,
	pages = {67–72},
	numpages = {6}
}

@book{aima,
	author = {Russell, Stuart and Norvig, Peter},
	title = {Artificial Intelligence: A Modern Approach},
	year = {2009},
	isbn = {0136042597},
	publisher = {Prentice Hall Press},
	address = {USA},
	edition = {3rd},
	abstract = {The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.}
}

@inbook{eda,
	author = {Komorowski, Matthieu and Marshall, Dominic and Salciccioli, Justin and Crutain, Yves},
	year = {2016},
	month = {09},
	pages = {185-203},
	title = {Exploratory Data Analysis},
	isbn = {978-3-319-43740-8},
	journal = {Secondary Analysis of Electronic Health Records},
	doi = {10.1007/978-3-319-43742-2_15}
}

@article{Mariscal_Marbán_Fernández_2010, title={A survey of data mining and knowledge discovery process models and methodologies}, volume={25}, DOI={10.1017/S0269888910000032}, number={2}, journal={The Knowledge Engineering Review}, author={Mariscal, Gonzalo and Marbán, Óscar and Fernández, Covadonga}, year={2010}, pages={137–166}}

@misc{datasciencepmCRISPDMStill,
	author = {Jeff Saltz},
	title = {{C}{R}{I}{S}{P}-{D}{M} is {S}till the {M}ost {P}opular {F}ramework for {E}xecuting {D}ata {S}cience {P}rojects - {D}ata {S}cience {P}{M} --- datascience-pm.com},
	howpublished = {\url{https://www.datascience-pm.com/crisp-dm-still-most-popular/}},
	year = {2024},
	note = {[Accessed 28-05-2025]},
}

@book{mitchell1997machine,
	added-at = {2022-09-08T09:41:39.000+0200},
	author = {Mitchell, Tom M},
	biburl = {https://www.bibsonomy.org/bibtex/2ea9f893d9d19c182bcf2822eb590fe4f/msteininger},
	interhash = {479a66c32badb3a455fbdcf8e6633a5d},
	intrahash = {ea9f893d9d19c182bcf2822eb590fe4f},
	keywords = {book ml},
	number = 9,
	publisher = {McGraw-hill New York},
	timestamp = {2022-09-08T17:32:10.000+0200},
	title = {Machine learning},
	volume = 1,
	year = 1997
}

@book{mlprobabilistic,
	author = {Murphy, Kevin P.},
	title = {Machine Learning: A Probabilistic Perspective},
	year = {2012},
	isbn = {0262018020},
	publisher = {The MIT Press},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.}
}

@misc{tai2021surveyregressionalgorithmsconnections,
	title={A Survey Of Regression Algorithms And Connections With Deep Learning}, 
	author={Yunpeng Tai},
	year={2021},
	eprint={2104.12647},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2104.12647}, 
}

@book{Goodfellow-et-al-2016,
	added-at = {2017-03-13T20:27:27.000+0100},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	biburl = {https://www.bibsonomy.org/bibtex/2175f81afff897a68829e4d30c080a8fb/hotho},
	interhash = {62814dec510d5c55b0b38ad85a6c748d},
	intrahash = {175f81afff897a68829e4d30c080a8fb},
	keywords = {book deep learning toread},
	note = {Book in preparation for MIT Press},
	publisher = {MIT Press},
	timestamp = {2017-04-14T13:44:20.000+0200},
	title = {Deep Learning},
	url = {http://www.deeplearningbook.org},
	year = 2016
}

@book{patternrecognition,
	author = {Bishop, Christopher M.},
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	year = {2006},
	isbn = {0387310738},
	publisher = {Springer-Verlag},
	address = {Berlin, Heidelberg}
}

@book{Burkov2019TheHM,
	title={The Hundred-Page Machine Learning Book},
	author={Andriy Burkov},
	year={2019},
	url={https://api.semanticscholar.org/CorpusID:202780305}
}

@inproceedings{l1l2,
	author = {Ng, Andrew Y.},
	title = {Feature selection, L1 vs. L2 regularization, and rotational invariance},
	year = {2004},
	isbn = {1581138385},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1015330.1015435},
	doi = {10.1145/1015330.1015435},
	abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
	booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
	pages = {78},
	location = {Banff, Alberta, Canada},
	series = {ICML '04}
}

@article{lasso,
	ISSN = {00359246},
	URL = {http://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	author = {Robert Tibshirani},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {1},
	pages = {267--288},
	publisher = {[Royal Statistical Society, Oxford University Press]},
	title = {Regression Shrinkage and Selection via the Lasso},
	urldate = {2025-05-30},
	volume = {58},
	year = {1996}
}

@article{elasticnet,
	author = {Zou, Hui and Hastie, Trevor},
	title = {Regularization and Variable Selection Via the Elastic Net},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	volume = {67},
	number = {2},
	pages = {301-320},
	year = {2005},
	month = {03},
	abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
	issn = {1369-7412},
	doi = {10.1111/j.1467-9868.2005.00503.x},
	url = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
	eprint = {https://academic.oup.com/jrsssb/article-pdf/67/2/301/49795094/jrsssb\_67\_2\_301.pdf},
}

@article{ridge,
	ISSN = {00401706},
	URL = {http://www.jstor.org/stable/1271436},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	author = {Arthur E. Hoerl and Robert W. Kennard},
	journal = {Technometrics},
	number = {1},
	pages = {80--86},
	publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
	title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
	urldate = {2025-05-30},
	volume = {42},
	year = {2000}
}

@article{svr,
	author = {Smola, Alex and Schölkopf, Bernhard},
	year = {2004},
	month = {08},
	pages = {199-222},
	title = {A tutorial on support vector regression},
	volume = {14},
	journal = {Statistics and Computing},
	doi = {10.1023/B%3ASTCO.0000035301.49549.88}
}




